{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_excel('Train.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature List Operations\n",
    "0. sor: One-hot dummy\n",
    "1. cdf_seq_no: Deleted, unique to transaction\n",
    "2. trans_desc: Tokenized and Word2Vec\n",
    "3. merchant_cat_code: Numerical input\n",
    "4. amt: Input (continuous)                       \n",
    "5. db_cr_cd: One-hot dummy\n",
    "6. payment_reporting_category \n",
    "7. payment_category:  One-hot dummy\n",
    "8. is_internationa: One-hot dummy\n",
    "9. default_brand: Word2Vec sum embedding\n",
    "10. default_location: Turned last two to State Categorical\n",
    "11. qrated_brand: unused\n",
    "12. coalesced_brand: Word2Vec sum embedding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused columns\n",
    "df_train = df_raw.drop(columns=['cdf_seq_no','payment_reporting_category'])\n",
    "\n",
    "# Convert categorical values to binary indicators (one-hot)\n",
    "categ = ['sor','db_cr_cd', 'payment_category', 'is_international']\n",
    "df_categ = pd.concat([\n",
    "    df_train.drop(columns=categ), # dataset without the categorical features\n",
    "    pd.get_dummies(df_train[categ], columns=categ, drop_first=False) # categorical features converted to dummies\n",
    "], axis=1)\n",
    "\n",
    "# Takes State from location (with some errors)\n",
    "df_categ['State'] = pd.factorize(df_categ['default_location'].str[-2:])[0]\n",
    "\n",
    "\n",
    "df_categ.fillna(0, inplace=True)\n",
    "\n",
    "# Turn targets into numeric classes\n",
    "[df_categ['Category'],class_names] = pd.factorize(df_categ.Category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Word2Vec Model\n",
    "def load_glove_model(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    glove_model = {}\n",
    "    with open(File,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "    print(f\"{len(glove_model)} words loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "glove_word2vec = load_glove_model('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000 words loaded!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Toekenize and word2Vec \n",
    "\n",
    "embed_size = 50\n",
    "num_embed = 2\n",
    "embed = np.zeros((len(df_categ), embed_size*num_embed))\n",
    "\n",
    "for iRow in range(len(df_categ)):\n",
    "    #default_brand\n",
    "    brand_token = np.zeros(embed_size)\n",
    "    for iToken in str(df_categ.iloc[iRow]['default_brand']).lower().split():\n",
    "        if iToken in glove_word2vec:\n",
    "            brand_token += glove_word2vec[iToken]\n",
    "    \n",
    "    coalesced_token = np.zeros(embed_size)\n",
    "    for iToken in str(df_categ.iloc[iRow]['coalesced_brand']).lower().split():\n",
    "        if iToken in glove_word2vec:\n",
    "            coalesced_token += glove_word2vec[iToken]\n",
    "\n",
    "    embed[iRow] = np.concatenate((brand_token, coalesced_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Feature Data: (40000, 114)\n",
      "Shape of Target Data: (40000,)\n",
      "Data saved to data.npz\n"
     ]
    }
   ],
   "source": [
    "X_data = np.hstack((df_categ.iloc[:, [1,2]].to_numpy(),df_categ.iloc[:, 8:].to_numpy(), embed))\n",
    "y_data = df_categ.iloc[:, 7].to_numpy()\n",
    "\n",
    "print('Shape of Feature Data: {}'.format(X_data.shape))\n",
    "print('Shape of Target Data: {}'.format(y_data.shape))\n",
    "\n",
    "np.savez('data.npz', X_data=X_data, y_data=y_data)\n",
    "print('Data saved to data.npz')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bd648419a684b263b947e63584b9a1ef9aefb5ec8226dde005080c7f6eb6cea"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 ('nlp_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
